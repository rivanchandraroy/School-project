{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import pytz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Date range correction\n",
    "* Only dates after July 2023 will be taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedf776a2ede4ad08d1fc15907a16bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing homes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing R1.csv\n",
      "==================================================\n",
      "Processing R2.csv\n",
      "==================================================\n",
      "Processing R3.csv\n",
      "==================================================\n",
      "Processing R4.csv\n",
      "==================================================\n",
      "Processing SU1.csv\n",
      "==================================================\n",
      "Processing SU2.csv\n",
      "==================================================\n",
      "Processing U1.csv\n",
      "==================================================\n",
      "Processing U2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dipankar Mitra\\AppData\\Local\\Temp\\ipykernel_24152\\2167184787.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filedir, parse_dates=['UTCDateTime'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing U3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dipankar Mitra\\AppData\\Local\\Temp\\ipykernel_24152\\2167184787.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filedir, parse_dates=['UTCDateTime'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing U4.csv\n"
     ]
    }
   ],
   "source": [
    "# Hourly average time shiftinhg\n",
    "dir = \"merged\"\n",
    "files = [d for d in os.listdir(dir) if d.endswith('.csv')]\n",
    "# subdirs = [\"Badda Home\"]\n",
    "for file in tqdm(files, desc=\"Processing homes\"):\n",
    "    print('='*50)\n",
    "    print(f\"Processing {file}\")\n",
    "    filedir = os.path.join(dir, file)\n",
    "    df = pd.read_csv(filedir, parse_dates=['UTCDateTime'])\n",
    "    df[\"BDDateTime\"] = df['UTCDateTime'] + pd.Timedelta(hours=6)\n",
    "    df.to_csv(filedir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing R1.csv\n",
      "==================================================\n",
      "Processing R2.csv\n",
      "==================================================\n",
      "Processing R3.csv\n",
      "==================================================\n",
      "Processing R4.csv\n",
      "==================================================\n",
      "Processing SU1.csv\n",
      "==================================================\n",
      "Processing SU2.csv\n",
      "==================================================\n",
      "Processing U1.csv\n",
      "==================================================\n",
      "Processing U2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dipankar Mitra\\AppData\\Local\\Temp\\ipykernel_24152\\1109183770.py:8: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(dir, file), parse_dates=['BDDateTime'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing U3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dipankar Mitra\\AppData\\Local\\Temp\\ipykernel_24152\\1109183770.py:8: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(dir, file), parse_dates=['BDDateTime'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing U4.csv\n"
     ]
    }
   ],
   "source": [
    "dir = \"merged\"\n",
    "files = [d for d in os.listdir(dir) if d.endswith('.csv')]\n",
    "# files = ['R4.csv']\n",
    "for file in files:\n",
    "    print('='*50)\n",
    "    print(f\"Processing {file}\")\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(dir, file), parse_dates=['BDDateTime'])\n",
    "    df = df[df['BDDateTime'] >= pd.to_datetime(\"2023-07-01\", format = \"%Y-%m-%d\")]\n",
    "    df = df[df['BDDateTime'] <= pd.to_datetime(\"2025-06-30\", format = \"%Y-%m-%d\")]\n",
    "    df.to_csv(os.path.join(dir, file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f97309b0884cc0bd699e5e7d9ae96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pricessing R1.csv\n",
      "File saved :  channel_combined\\R1.csv\n",
      "pricessing R2.csv\n",
      "File saved :  channel_combined\\R2.csv\n",
      "pricessing R3.csv\n",
      "File saved :  channel_combined\\R3.csv\n",
      "pricessing R4.csv\n",
      "File saved :  channel_combined\\R4.csv\n",
      "pricessing SU1.csv\n",
      "File saved :  channel_combined\\SU1.csv\n",
      "pricessing SU2.csv\n",
      "File saved :  channel_combined\\SU2.csv\n",
      "pricessing U1.csv\n",
      "File saved :  channel_combined\\U1.csv\n",
      "pricessing U2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dipankar Mitra\\AppData\\Local\\Temp\\ipykernel_24152\\3214687038.py:42: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(dir, file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved :  channel_combined\\U2.csv\n",
      "pricessing U3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dipankar Mitra\\AppData\\Local\\Temp\\ipykernel_24152\\3214687038.py:42: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(dir, file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping U3.csv\n",
      "pricessing U4.csv\n",
      "Skipping U4.csv\n"
     ]
    }
   ],
   "source": [
    "## merging channel\n",
    "\n",
    "# conditions =   {\n",
    "#     'R1' : 'avg',\n",
    "#     'R2' : 'b',\n",
    "#     'R3' : 'avg',\n",
    "#     'R4' : 'a',\n",
    "\n",
    "#     'SU1' : 'avg',\n",
    "#     'SU2' : 'avg',\n",
    "\n",
    "#     'U1' : 'avg',\n",
    "#     'U2' : 'a',\n",
    "#     'U3' : 'skip',\n",
    "#     'U4' : 'skip'\n",
    "# }\n",
    "\n",
    "conditions =   {\n",
    "    'R1' : 'avg',\n",
    "    'R2' : 'avg',\n",
    "    'R3' : 'avg',\n",
    "    'R4' : 'avg',\n",
    "\n",
    "    'SU1' : 'avg',\n",
    "    'SU2' : 'avg',\n",
    "\n",
    "    'U1' : 'avg',\n",
    "    'U2' : 'avg',\n",
    "    'U3' : 'skip',\n",
    "    'U4' : 'skip'\n",
    "}\n",
    "\n",
    "cols_to_combine = ['pm2_5_cf_1', 'pm2_5_atm', 'pm1_0_atm', 'pm10_0_atm']\n",
    "\n",
    "## average calculation\n",
    "dir = \"merged\"\n",
    "target = 'channel_combined'\n",
    "files = [d for d in os.listdir(dir) if d.endswith('csv')]\n",
    "for file in tqdm(files):\n",
    "    print(f'pricessing {file}')\n",
    "\n",
    "    df = pd.read_csv(os.path.join(dir, file))\n",
    "    cond = conditions[file.split('.')[0]]\n",
    "    if cond == 'skip':\n",
    "        print(f'Skipping {file}')\n",
    "        continue\n",
    "    for col in cols_to_combine:\n",
    "        if cond == 'avg':\n",
    "            df[f'{col}_comb'] = (df[col] + df[f'{col}_b'])/2\n",
    "        elif cond == 'a':\n",
    "            df[f'{col}_comb'] = df[col]\n",
    "        elif cond == \"b\":\n",
    "            df[f'{col}_comb'] = df[f'{col}_b']\n",
    "        else:\n",
    "            raise ValueError(\"cond value not recognized\")\n",
    "\n",
    "    necessary_cols = ['BDDateTime', 'current_temp_f', 'current_humidity', 'current_dewpoint_f', 'pressure', 'pm1_0_atm_comb', 'pm2_5_atm_comb', 'pm10_0_atm_comb', 'pm2_5_cf_1_comb']\n",
    "    df = df[necessary_cols].copy()\n",
    "    target_file_dir = os.path.join(target, file)\n",
    "    df.to_csv(target_file_dir, index=False)\n",
    "    print(\"File saved : \", target_file_dir)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Appling calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a44902412e349fbaa24cb62e9c89c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing R1.csv\n",
      "Saved R1.csv\n",
      "processing R2.csv\n",
      "Saved R2.csv\n",
      "processing R3.csv\n",
      "Saved R3.csv\n",
      "processing R4.csv\n",
      "Saved R4.csv\n",
      "processing SU1.csv\n",
      "Saved SU1.csv\n",
      "processing SU2.csv\n",
      "Saved SU2.csv\n",
      "processing U1.csv\n",
      "Saved U1.csv\n",
      "processing U2.csv\n",
      "Saved U2.csv\n"
     ]
    }
   ],
   "source": [
    "## calibration\n",
    "## (thersholds were applied once again here)\n",
    "\n",
    "def calibrate_cf_1(time, pm2_5):\n",
    "    if pd.isnull(time) or pd.isnull(pm2_5):\n",
    "        return float('nan')\n",
    "    month = time.month\n",
    "    if month < 4 or month > 9:\n",
    "        res =  0.63*pm2_5 - 1.1\n",
    "    else:\n",
    "        res =  0.77 * pm2_5 + 6.4\n",
    "    \n",
    "    if res >= 5 and res <= 500:\n",
    "        return res\n",
    "    else:\n",
    "        return float('nan')\n",
    "    \n",
    "def calibrate_atm(time, pm2_5):\n",
    "    if pd.isnull(time) or pd.isnull(pm2_5):\n",
    "        return float('nan')\n",
    "    month = time.month\n",
    "    if month < 4 or month > 9:\n",
    "        res =  0.96*pm2_5 - 2.0\n",
    "    else:\n",
    "        res = 1.4 * pm2_5 - 10.23\n",
    "\n",
    "    if res >= 5 and res <= 500:\n",
    "        return res\n",
    "    else:\n",
    "        return float('nan')\n",
    "    \n",
    "def limit_data(time, data):\n",
    "    if pd.isnull(time) or pd.isnull(data):\n",
    "        return float('nan')\n",
    "    if data >= 5 and data <= 500:\n",
    "        return data\n",
    "    else:\n",
    "        return float('nan')    \n",
    "\n",
    "\n",
    "## average calculation\n",
    "dir = \"channel_combined\"\n",
    "target = 'calibrated_2min'\n",
    "files = [f for f in os.listdir(dir) if f.endswith('csv')]\n",
    "for file in tqdm(files):\n",
    "    print(f'processing {file}')\n",
    "        \n",
    "    df = pd.read_csv(os.path.join(dir, file))\n",
    "    df['BDDateTime'] = pd.to_datetime(df['BDDateTime'])\n",
    "\n",
    "    df['pm2_5_cf_1_calibrated'] = df.apply(\n",
    "        lambda row : calibrate_cf_1(row['BDDateTime'], row['pm2_5_cf_1_comb']), axis = 1\n",
    "    )\n",
    "    df['pm2_5_atm_calibrated'] = df.apply(\n",
    "        lambda row : calibrate_atm(row['BDDateTime'], row['pm2_5_atm_comb']), axis = 1\n",
    "    )\n",
    "    df['pm1_0_atm_comb'] = df.apply(\n",
    "        lambda row : limit_data(row['BDDateTime'], row['pm1_0_atm_comb']), axis = 1\n",
    "    )\n",
    "    df['pm10_0_atm_comb'] = df.apply(\n",
    "        lambda row : limit_data(row['BDDateTime'], row['pm10_0_atm_comb']), axis = 1\n",
    "    )\n",
    "\n",
    "    df.loc[(df['pm2_5_atm_calibrated'] <= 0) | (df['pm2_5_atm_calibrated'] > 500), 'pm2_5_atm_calibrated'] = float('nan')\n",
    "    df.loc[(df['pm2_5_cf_1_calibrated'] <= 0) | (df['pm2_5_cf_1_calibrated'] > 500), 'pm2_5_cf_1_calibrated'] = float('nan')\n",
    "\n",
    "    df.drop(columns=['pm2_5_atm_comb', 'pm2_5_cf_1_comb'], inplace=True)\n",
    "    df.rename(columns={\n",
    "        'pm2_5_atm_calibrated' : 'pm2_5_atm',\n",
    "        'pm2_5_cf_1_calibrated' : 'pm2_5_cf_1',\n",
    "        'pm1_0_atm_comb' : 'pm1_0_atm',\n",
    "        'pm10_0_atm_comb' : 'pm10_0_atm'\n",
    "    }, inplace=True)\n",
    "\n",
    "    df.to_csv(os.path.join(target, file), index=False)\n",
    "    print(\"Saved\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hourly average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_averages(df, datas, time='UTCDateTime', name = None):\n",
    "    results = {\"time\" : []}\n",
    "    for col in datas:\n",
    "        results[col] = []\n",
    "    for col in datas:\n",
    "        results[col+\"count\"] = []\n",
    "    \n",
    "\n",
    "    # Ensure the DataFrame has the correct time and data columns\n",
    "    df = df[datas + [time]].copy()\n",
    "    # df = df.dropna()  # Remove rows with NaN in time or data columns\n",
    "    df[time] = pd.to_datetime(df[time])  # Ensure time column is datetime\n",
    "    df = df.sort_values(by=time)  # Sort by time column\n",
    "\n",
    "    # Generate specific hourly times for calculation (00:30, 01:30, ..., 23:30)\n",
    "    start_time = df[time].min().replace(minute=30, second=0, microsecond=0)\n",
    "    end_time = df[time].max().replace(minute=30, second=0, microsecond=0)\n",
    "    hourly_times = pd.date_range(start=start_time, end=end_time, freq=\"1h\")\n",
    "\n",
    "    for target_time in tqdm(hourly_times, desc=f\"Taking hourly averages : {name}\"):\n",
    "        # Append the results\n",
    "        results[\"time\"].append(target_time)\n",
    "        # Define the 1-hour window (30 minutes before and after)\n",
    "        start_window = target_time - pd.Timedelta(minutes=30)\n",
    "        end_window = target_time + pd.Timedelta(minutes=30)\n",
    "        # Filter data within the window\n",
    "        window_data_all = df[(df[time] >= start_window) & (df[time] <= end_window)][datas]\n",
    "        for data in datas:\n",
    "            # Count the number of data points in the window\n",
    "            window_data = window_data_all[data].dropna()\n",
    "            count = len(window_data)\n",
    "            # Compute the average if the number of points is >= 10\n",
    "            if count >= 23:\n",
    "                avg = window_data.mean()\n",
    "            else:\n",
    "                avg = np.nan  # Fill with NaN if insufficient data\n",
    "            \n",
    "            results[data].append(avg)\n",
    "            results[data+'count'].append(count)\n",
    "\n",
    "    # Convert results to a DataFrame for convenience\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790293a7776d426aa3062031eba443e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing homes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing R1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdaa7de701b244ed8869398aa5739291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking hourly averages : R1.csv:   0%|          | 0/16885 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R1.csv\n",
      "==================================================\n",
      "Processing R2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b2329ea1574aed94259562894e4669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking hourly averages : R2.csv:   0%|          | 0/17514 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R2.csv\n",
      "==================================================\n",
      "Processing R3.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043748f772c943f197ac527aadb762ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking hourly averages : R3.csv:   0%|          | 0/16978 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R3.csv\n",
      "==================================================\n",
      "Processing R4.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a91b026ec04b6dac72ec7e49b0d9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking hourly averages : R4.csv:   0%|          | 0/17405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R4.csv\n",
      "==================================================\n",
      "Processing SU1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4090d3f21d864c8f9bdbf8faa8af53d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking hourly averages : SU1.csv:   0%|          | 0/16520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : SU1.csv\n",
      "==================================================\n",
      "Processing SU2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6ca7b896804833ac4049b5e57a9e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking hourly averages : SU2.csv:   0%|          | 0/16520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : SU2.csv\n",
      "==================================================\n",
      "Processing U1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56122ce15a414055ba765e070d45557b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking hourly averages : U1.csv:   0%|          | 0/16955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : U1.csv\n",
      "==================================================\n",
      "Processing U2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a813cbfb48274a5487a42ce875d23a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking hourly averages : U2.csv:   0%|          | 0/17098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : U2.csv\n"
     ]
    }
   ],
   "source": [
    "# Hourly average\n",
    "dir = \"calibrated_2min\"\n",
    "files = [f for f in os.listdir(dir) if f.endswith('.csv')]\n",
    "for file in tqdm(files, desc=\"Processing homes\"):\n",
    "    print('='*50)\n",
    "    print(f\"Processing {file}\")\n",
    "\n",
    "    if False and os.path.exists(os.path.join(dir, subdir, f'Hourly_average_{type}.csv')):\n",
    "        print(\"file found. skipping\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(dir, file), parse_dates=['BDDateTime'])\n",
    "    hourly_avg = hourly_averages(df, datas=['current_temp_f', 'current_humidity', 'current_dewpoint_f', 'pressure', \"pm1_0_atm\", 'pm2_5_atm', \"pm10_0_atm\", \"pm2_5_cf_1\"], time='BDDateTime', name = file)\n",
    "    hourly_avg.to_csv(os.path.join('hourly_averages', file), index=False)\n",
    "    print(f\"saved : {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_averages(df, datas, time='UTCDateTime', name=None):\n",
    "    \"\"\"\n",
    "    Generate daily averages from an hourly-averaged dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing hourly averages with a time column and one column per data in `datas`.\n",
    "    datas : list[str]\n",
    "        Names of the data columns to average daily (same as used in hourly_averages).\n",
    "    time : str, default 'UTCDateTime'\n",
    "        Name of the datetime column.\n",
    "    name : str or None\n",
    "        Optional label for the tqdm progress bar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "          - 'time' (daily timestamps at 00:00)\n",
    "          - one column per data in `datas` (daily average, NaN if <= 12 hourly points available)\n",
    "          - one '{data}count' column per data with the number of hourly points used that day\n",
    "    \"\"\"\n",
    "    results = {\"time\": []}\n",
    "    for col in datas:\n",
    "        results[col] = []\n",
    "    for col in datas:\n",
    "        results[col + \"count\"] = []\n",
    "\n",
    "    # Ensure time is datetime and sorted\n",
    "    df = df.copy()\n",
    "    df[time] = pd.to_datetime(df[time])\n",
    "    df = df.sort_values(by=time)\n",
    "\n",
    "    # Build daily range from min to max day (normalized to midnight)\n",
    "    start_day = df[time].min().normalize()\n",
    "    end_day = df[time].max().normalize()\n",
    "    daily_days = pd.date_range(start=start_day, end=end_day, freq=\"1D\")\n",
    "\n",
    "    for day in tqdm(daily_days, desc=f\"Taking daily averages : {name}\"):\n",
    "        # Daily window: [00:00, next day 00:00)\n",
    "        start_window = day\n",
    "        end_window = day + pd.Timedelta(days=1)\n",
    "\n",
    "        results[\"time\"].append(day)\n",
    "\n",
    "        window_df = df[(df[time] >= start_window) & (df[time] < end_window)]\n",
    "\n",
    "        for data in datas:\n",
    "            series = window_df[data].dropna()\n",
    "            count = len(series)\n",
    "\n",
    "            # Require > 12 hourly points to compute daily average\n",
    "            if count > 12:\n",
    "                avg = series.mean()\n",
    "            else:\n",
    "                avg = np.nan\n",
    "\n",
    "            results[data].append(avg)\n",
    "            results[data + \"count\"].append(count)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d9a7ec52cd48c6a60911ccd831dc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Schools:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing R1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6824768fa0d460391032dce421b648e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking daily averages : R1.csv:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R1.csv\n",
      "==================================================\n",
      "Processing R2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a56d374590b4558b68a53a2a8e7e04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking daily averages : R2.csv:   0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R2.csv\n",
      "==================================================\n",
      "Processing R3.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824788c97521461fa939fab2d7c56c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking daily averages : R3.csv:   0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R3.csv\n",
      "==================================================\n",
      "Processing R4.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40a4940cd7b48228ed2c1dda3e6823e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking daily averages : R4.csv:   0%|          | 0/726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R4.csv\n",
      "==================================================\n",
      "Processing SU1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66f44048c8743f09aff7b0b4d6827c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking daily averages : SU1.csv:   0%|          | 0/689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : SU1.csv\n",
      "==================================================\n",
      "Processing SU2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe83dec9bfc4525abe9716d2eaa2567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking daily averages : SU2.csv:   0%|          | 0/689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : SU2.csv\n",
      "==================================================\n",
      "Processing U1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259ae7b51dba40268ea7d7464ae09a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking daily averages : U1.csv:   0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : U1.csv\n",
      "==================================================\n",
      "Processing U2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db167c5a339e43ce83befa5a7089dd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking daily averages : U2.csv:   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : U2.csv\n"
     ]
    }
   ],
   "source": [
    "# Daily average\n",
    "dir = \"hourly_averages\"\n",
    "target = 'daily_averages'\n",
    "files = [d for d in os.listdir(dir) if d.endswith('.csv')]\n",
    "for file in tqdm(files, desc=\"Processing Schools\"):\n",
    "    print('='*50)\n",
    "    print(f\"Processing {file}\")\n",
    "    if False and os.path.exists(os.path.join(dir, subdir, f'Hourly_average_{type}.csv')):\n",
    "        print(\"file found. skipping\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(dir, file))\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    daily_avg = daily_averages(df, datas=['current_temp_f', 'current_humidity', 'current_dewpoint_f', 'pressure', \"pm1_0_atm\", 'pm2_5_atm', \"pm10_0_atm\", \"pm2_5_cf_1\"], time='time', name = file)\n",
    "    daily_avg.to_csv(os.path.join(target, file), index=False)\n",
    "    # daily_avg.info()\n",
    "    print(f\"saved : {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month avergae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_averages(df, datas, time='UTCDateTime', name=None):\n",
    "    \"\"\"\n",
    "    Generate monthly averages from a daily-averaged dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing daily averages with a time column and one column per data in `datas`.\n",
    "    datas : list[str]\n",
    "        Names of the data columns to average monthly (same as used in daily_averages).\n",
    "    time : str, default 'UTCDateTime'\n",
    "        Name of the datetime column.\n",
    "    name : str or None\n",
    "        Optional label for the tqdm progress bar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "          - 'time' (monthly timestamps at the first day of the month, 00:00)\n",
    "          - one column per data in `datas` (monthly average, None if < 15 days available)\n",
    "          - one '{data}count' column per data with the number of daily points used that month\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\"time\": []}\n",
    "    for col in datas:\n",
    "        results[col] = []\n",
    "    for col in datas:\n",
    "        results[col + \"count\"] = []\n",
    "\n",
    "    # Ensure time is datetime and sorted\n",
    "    df = df.copy()\n",
    "    df[time] = pd.to_datetime(df[time])\n",
    "    df = df.sort_values(by=time)\n",
    "\n",
    "    # Build list of month starts (MS = Month Start)\n",
    "    start_month = df[time].min().to_period('M').to_timestamp(how='start')\n",
    "    end_month = df[time].max().to_period('M').to_timestamp(how='start')\n",
    "    month_starts = pd.date_range(start=start_month, end=end_month, freq='MS')\n",
    "\n",
    "    for ms in tqdm(month_starts, desc=f\"Taking monthly averages : {name}\"):\n",
    "        # Monthly window: [month_start, next_month_start)\n",
    "        next_ms = (ms + pd.offsets.MonthBegin(1))\n",
    "        window_df = df[(df[time] >= ms) & (df[time] < next_ms)]\n",
    "\n",
    "        results[\"time\"].append(ms)\n",
    "\n",
    "        for data in datas:\n",
    "            series = window_df[data].dropna()\n",
    "            count = int(series.shape[0])\n",
    "\n",
    "            # Require at least 15 daily points to compute monthly average\n",
    "            if count >= 15:\n",
    "                avg = float(series.mean())\n",
    "            else:\n",
    "                avg = None  # keep null if insufficient data\n",
    "\n",
    "            results[data].append(avg)\n",
    "            results[data + \"count\"].append(count)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea714134b534c749f41211c8401f236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Schools:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing R1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae766b56146747d4a98c521c939eb8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking monthly averages : R1.csv:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R1.csv\n",
      "==================================================\n",
      "Processing R2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ab1fe8fee848068b3fdbfefa431a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking monthly averages : R2.csv:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R2.csv\n",
      "==================================================\n",
      "Processing R3.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6524f879db15442c80f221411cfeb1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking monthly averages : R3.csv:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R3.csv\n",
      "==================================================\n",
      "Processing R4.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6757130dd1e747d0aa731aaf569944a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking monthly averages : R4.csv:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R4.csv\n",
      "==================================================\n",
      "Processing SU1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e582b4a75d974f5fa7d4e829ae11d7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking monthly averages : SU1.csv:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : SU1.csv\n",
      "==================================================\n",
      "Processing SU2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7755090a97ca40c5b7e08b8996d54855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking monthly averages : SU2.csv:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : SU2.csv\n",
      "==================================================\n",
      "Processing U1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5b7cb10d234878a2284fd1523a325c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking monthly averages : U1.csv:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : U1.csv\n",
      "==================================================\n",
      "Processing U2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b9db2a1ba64b46ad6b7c2686bfaa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking monthly averages : U2.csv:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : U2.csv\n"
     ]
    }
   ],
   "source": [
    "# Monthly average\n",
    "dir = \"daily_averages\"\n",
    "target = 'monthly_averages'\n",
    "files = [d for d in os.listdir(dir) if d.endswith('.csv')]\n",
    "for file in tqdm(files, desc=\"Processing Schools\"):\n",
    "    print('='*50)\n",
    "    print(f\"Processing {file}\")\n",
    "    if False and os.path.exists(os.path.join(dir, subdir, f'Hourly_average_{type}.csv')):\n",
    "        print(\"file found. skipping\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(dir, file))\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    monthly_avg = monthly_averages(df, datas=['current_temp_f', 'current_humidity', 'current_dewpoint_f', 'pressure', \"pm1_0_atm\", 'pm2_5_atm', \"pm10_0_atm\", \"pm2_5_cf_1\"], time='time', name = file)\n",
    "    monthly_avg.to_csv(os.path.join(target, file), index=False)\n",
    "    # monthly_avg.info()\n",
    "    print(f\"saved : {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_averages(df, datas, time='UTCDateTime', name=None):\n",
    "    \"\"\"\n",
    "    Generate fiscal-year (Jul 1 → Jun 30) averages from a monthly-averaged dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing monthly averages with a time column and one column per data in `datas`.\n",
    "    datas : list[str]\n",
    "        Names of the data columns to average yearly (same as used in monthly_averages).\n",
    "    time : str, default 'UTCDateTime'\n",
    "        Name of the datetime column.\n",
    "    name : str or None\n",
    "        Optional label for the tqdm progress bar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "          - 'time' (fiscal-year start timestamps at July 1st, 00:00)\n",
    "          - one column per data in `datas` (yearly average, None if < 9 monthly points in the fiscal year)\n",
    "          - one '{data}count' column per data with the number of monthly points used that fiscal year\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\"time\": []}\n",
    "    for col in datas:\n",
    "        results[col] = []\n",
    "    for col in datas:\n",
    "        results[col + \"count\"] = []\n",
    "\n",
    "    # Ensure time is datetime and sorted\n",
    "    df = df.copy()\n",
    "    df[time] = pd.to_datetime(df[time])\n",
    "    df = df.sort_values(by=time)\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    # Helper: map any date to its fiscal-year start (July 1)\n",
    "    def fiscal_start(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "        year = ts.year if ts.month >= 7 else ts.year - 1\n",
    "        return pd.Timestamp(year=year, month=7, day=1)\n",
    "\n",
    "    # Build range of fiscal-year starts from min to max\n",
    "    start_fy = fiscal_start(df[time].min())\n",
    "    end_fy = fiscal_start(df[time].max())\n",
    "    fy_starts = pd.date_range(start=start_fy, end=end_fy, freq='YS-JUL')  # Annual Start in July\n",
    "\n",
    "    # Offset for next fiscal-year start\n",
    "    fy_step = pd.offsets.YearBegin(1, month=7)\n",
    "\n",
    "    for fy in tqdm(fy_starts, desc=f\"Taking yearly (Jul-Jun) averages : {name}\"):\n",
    "        next_fy = fy + fy_step\n",
    "        window_df = df[(df[time] >= fy) & (df[time] < next_fy)]\n",
    "\n",
    "        results[\"time\"].append(fy)\n",
    "\n",
    "        for data in datas:\n",
    "            series = window_df[data].dropna()\n",
    "            count = int(series.shape[0])\n",
    "\n",
    "            # Require at least 9 monthly points (~75%) within the fiscal year\n",
    "            if count >= 6:\n",
    "                avg = float(series.mean())\n",
    "            else:\n",
    "                avg = None  # insufficient data for fiscal year average\n",
    "\n",
    "            results[data].append(avg)\n",
    "            results[data + \"count\"].append(count)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387067b4d0104efa84fc1a66fb481125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Schools:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing R1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d5a6478ab44410bcee2583d46235db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking yearly (Jul-Jun) averages : R1.csv:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R1.csv\n",
      "==================================================\n",
      "Processing R2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7e2bf593f84b3891c37a28dcd7b8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking yearly (Jul-Jun) averages : R2.csv:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R2.csv\n",
      "==================================================\n",
      "Processing R3.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fe83acce8746fa83471527b4229419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking yearly (Jul-Jun) averages : R3.csv:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R3.csv\n",
      "==================================================\n",
      "Processing R4.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f342ca4053453ca9e690c533593783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking yearly (Jul-Jun) averages : R4.csv:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : R4.csv\n",
      "==================================================\n",
      "Processing SU1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbefcf6d3c74c5699df7ab99ed0ac28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking yearly (Jul-Jun) averages : SU1.csv:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : SU1.csv\n",
      "==================================================\n",
      "Processing SU2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf1aeaf5baf4e98929ac82fa7e30e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking yearly (Jul-Jun) averages : SU2.csv:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : SU2.csv\n",
      "==================================================\n",
      "Processing U1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3fb87b31434194b1d5117b06c75585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking yearly (Jul-Jun) averages : U1.csv:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : U1.csv\n",
      "==================================================\n",
      "Processing U2.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd58f3798b274ea7a185674ab7292619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Taking yearly (Jul-Jun) averages : U2.csv:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved : U2.csv\n"
     ]
    }
   ],
   "source": [
    "# Monthly average\n",
    "dir = \"monthly_averages\"\n",
    "target = 'yearly_averages'\n",
    "files = [d for d in os.listdir(dir) if d.endswith('.csv')]\n",
    "for file in tqdm(files, desc=\"Processing Schools\"):\n",
    "    print('='*50)\n",
    "    print(f\"Processing {file}\")\n",
    "    if False and os.path.exists(os.path.join(dir, subdir, f'Hourly_average_{type}.csv')):\n",
    "        print(\"file found. skipping\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(dir, file))\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    yearly_average = yearly_averages(df, datas=['current_temp_f', 'current_humidity', 'current_dewpoint_f', 'pressure', \"pm1_0_atm\", 'pm2_5_atm', \"pm10_0_atm\", \"pm2_5_cf_1\"], time='time', name = file)\n",
    "    yearly_average.to_csv(os.path.join(target, file), index=False)\n",
    "    # yearly_average.info()\n",
    "    print(f\"saved : {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(all_in_one)",
   "language": "python",
   "name": "all_in_one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
